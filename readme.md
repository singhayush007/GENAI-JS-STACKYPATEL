# Generative AI with Node.js, LangChain, TypeScript & Open AI & Google Gemini

This repository contains my learning and practice code while studying **Generative AI with Node.js and TypeScript**.
The implementation uses **Google Gemini API** **OpenAI**, inspired by Stacky Patel Generative AI course.

---

## ğŸ“š Lecture 01 â€“ Introduction & Setup

### ğŸ”§ What You'll Learn in This Lecture:

âœ… How to get and use your **Google Gemini API key**  
âœ… Understanding **Google Gemini services and models**  
âœ… Setting up a **Node.js project with TypeScript**  
âœ… Installing and using the **Google Generative AI (Gemini) Node.js SDK**  
âœ… Best practices for organising a **Generative AI project**

---

## ğŸ“š Lecture 02 â€“ Gemini Chat Application (CLI Based)

### ğŸ”§ What You'll Learn in This Lecture:

âœ… Setting up **Google Gemini client** using Node.js  
âœ… Loading environment variables securely with **dotenv**  
âœ… Understanding **Gemini models** (`gemini-flash-latest`)  
âœ… Using **system instructions** to control AI behaviour  
âœ… Creating a **chat-based conversation** using Gemini  
âœ… Taking real-time user input from terminal  
âœ… Handling errors and graceful exit in a GenAI app

---

## ğŸ“š Lecture 03 â€“ Tool Calling with Gemini (Weather Fetch App)

### ğŸ”§ What You'll Learn in This Lecture:

âœ… Understanding **Tool Calling (Function Calling)** in Generative AI  
âœ… How Gemini decides **when to call a tool** vs when to respond normally  
âœ… Defining **custom tools / functions** for Gemini  
âœ… Implementing a **Weather Fetch Tool**  
âœ… Passing structured parameters (e.g. city name) from AI to function  
âœ… Executing real-world logic (API or mock data) from tool calls  
âœ… Returning tool results back to Gemini  
âœ… Complete flow: **User â†’ AI â†’ Tool â†’ AI â†’ Final Response**

### ğŸ› ï¸ What We Built:

- A CLI-based AI application where:
  - User asks questions like _"What is the weather in Delhi?"_
  - Gemini automatically triggers a **weather tool**
  - The tool fetches weather data
  - Gemini formats the data into a **human-friendly answer**

This lecture explains how **LLMs interact with external systems**, which is a **core concept for production-level GenAI apps**.

---

## ğŸ“š Lecture 04 â€“ Multimodal AI with DALLÂ·E & Whisper

### ğŸ”§ What You'll Learn in This Lecture:

âœ… Understanding **Multimodal AI** (Text, Image, Audio)  
âœ… Using **DALLÂ·E** for **Text-to-Image generation**  
âœ… Using **Whisper** for **Text-to-Speech (Voice generation)**  
âœ… Handling **binary outputs** (images & audio files) in Node.js  
âœ… Saving generated images and audio locally  
âœ… Managing TypeScript types and error handling  
âœ… Building real-world GenAI utilities using Node.js

### ğŸ› ï¸ What We Built:

#### ğŸ¨ Text to Image (DALLÂ·E)

- User provides a text prompt
- AI generates an image based on the prompt
- Image is saved locally (e.g. `.png` file)

#### ğŸ”Š Text to Voice (Whisper / TTS)

- User provides text input
- AI converts text into **natural-sounding speech**
- Audio file is generated and stored (e.g. `.mp3` / `.wav`)

This lecture demonstrates how GenAI goes **beyond chat**, enabling:

- Image generation
- Voice generation
- Creative & production-ready AI workflows

---

## ğŸ“š Lecture 05 â€“ Vector Embeddings & Similarity Search

### ğŸ”§ What You'll Learn in This Lecture:

âœ… What are **Vector Embeddings** and why they are important in GenAI  
âœ… Converting text into **numerical vector representations**  
âœ… Understanding **semantic meaning** through embeddings  
âœ… What is **Cosine Similarity** and how it works  
âœ… What is **Dot Product Similarity**  
âœ… Difference between **Cosine Similarity vs Dot Similarity**  
âœ… Measuring similarity between two pieces of text

### ğŸ§  Key Concepts Explained:

- **Vector Embeddings**

  - Text is converted into high-dimensional vectors
  - Similar meanings â†’ vectors closer to each other

- **Cosine Similarity**

  - Measures the **angle** between two vectors
  - Focuses on direction, not magnitude
  - Value ranges between `-1` and `1`
  - Commonly used in semantic search

- **Dot Product Similarity**
  - Measures similarity based on vector multiplication
  - Depends on both **direction and magnitude**
  - Faster but less normalized than cosine similarity

---

## ğŸ“š Lecture 06 â€“ Vector Databases with ChromaDB (Semantic Search)

### ğŸ”§ What You'll Learn in This Lecture:

âœ… What is a **Vector Database** and why it is needed  
âœ… How vector databases store and search embeddings  
âœ… Introduction to **ChromaDB**  
âœ… Creating and managing **collections** in ChromaDB  
âœ… Storing text data as **embeddings**  
âœ… Using metadata (role, ids) with vectors  
âœ… Performing **semantic similarity search**  
âœ… Understanding how vector DBs power **RAG systems**

### ğŸ§  Key Concepts Explained:

- **Vector Database**

  - A specialized database designed to store **vector embeddings**
  - Enables fast **similarity search** instead of exact matching
  - Used in semantic search, chat history memory, and RAG pipelines

- **ChromaDB**

  - Lightweight, open-source vector database
  - Easy to use with Node.js
  - Ideal for learning and local GenAI projects

- **Semantic Search**
  - Search is based on **meaning**, not keywords
  - User query is converted into an embedding
  - Closest vectors are returned using similarity metrics

## ğŸ“š Lecture 07 â€“ LangChain Fundamentals (Prompts, Batching & Chunking)

### ğŸ”§ What You'll Learn in This Lecture:

âœ… Introduction to **LangChain** and why it is used  
âœ… Setting up **ChatOpenAI** model with LangChain  
âœ… Using **PromptTemplate** for dynamic prompts  
âœ… Single LLM calls using LangChain  
âœ… Batch processing multiple prompts efficiently  
âœ… Handling long text using **Text Chunking**  
âœ… Understanding **RecursiveCharacterTextSplitter**  
âœ… Building structured and reusable LLM workflows

### ğŸ§  Key Concepts Explained:

- **LangChain**

  - A framework to build structured, modular GenAI applications
  - Simplifies prompt management, chaining, and LLM orchestration

- **PromptTemplate**

  - Allows dynamic prompt creation using variables
  - Helps maintain consistency and reusability in prompts

- **Batch Calls**

  - Send multiple prompts in one call
  - Improves performance and reduces overhead

- **Text Chunking**
  - Large text is split into smaller overlapping chunks
  - Prevents context length issues in LLMs

### ğŸ› ï¸ What We Built:

- Created a **ChatOpenAI** model using LangChain
- Built reusable prompts using **PromptTemplate**
- Executed:
  - **Single LLM calls**
  - **Batch LLM calls**
- Processed long text using **RecursiveCharacterTextSplitter**
- Sent chunked text to the LLM and generated explanations per chunk

This lecture focuses on **prompt engineering + scalability**, forming the base for advanced GenAI systems.

---

## ğŸ“š Lecture 08 â€“ Retrieval Augmented Generation (RAG) with LangChain

### ğŸ”§ What You'll Learn in This Lecture:

âœ… What is **Retrieval Augmented Generation (RAG)**  
âœ… Why RAG is needed over plain LLM responses  
âœ… Loading and processing external documents  
âœ… Chunking documents for better retrieval  
âœ… Creating **Embeddings** for documents  
âœ… Storing embeddings in **Chroma Vector Database**  
âœ… Performing **Similarity Search** using retrievers  
âœ… Combining retrieved context with LLM responses

### ğŸ§  Key Concepts Explained:

- **RAG (Retrieval Augmented Generation)**

  - Enhances LLMs with external knowledge
  - Prevents hallucinations
  - Answers are grounded in real documents

- **Retriever**

  - Fetches the most relevant document chunks
  - Uses vector similarity search

- **Context-Aware Prompting**
  - LLM is forced to answer using only retrieved context
  - If data is missing, model responds with `"I don't know"`

### ğŸ› ï¸ What We Built:

- Loaded a text document from local storage
- Split the document into overlapping chunks
- Generated embeddings using OpenAI Embeddings
- Stored vectors in **ChromaDB**
- Retrieved top-K relevant chunks for a query
- Passed retrieved context into a **RAG Prompt Template**
- Generated accurate, grounded answers using an LLM

### ğŸ” Example Queries:

- _"What is RAG?"_
- _"What is LangChain used for?"_
- _"Who invented Java?"_ (Correctly returns **"I don't know"**)

This lecture demonstrates a **production-grade GenAI pattern**, widely used in:

- AI chatbots
- Knowledge assistants
- Document Q&A systems

---

## ğŸ“š Lecture 09 â€“ AI Agent with PDF based RAG (Final Capstone Project)

### ğŸ”§ What You'll Learn in This Lecture:

âœ… Understanding **AI Agent style workflows**  
âœ… Loading and parsing **PDF documents**  
âœ… Extracting raw text from PDFs  
âœ… Splitting large documents into **overlapping chunks**  
âœ… Generating **vector embeddings** for each chunk  
âœ… Storing embeddings in a **Vector Database (ChromaDB)**  
âœ… Using **Retrievers** to fetch relevant document context  
âœ… Building a **RAG-based AI Agent** using LangChain  
âœ… Preventing hallucinations by grounding answers in documents  
âœ… Designing a **modular, production-style GenAI project**

---

### ğŸ§  Key Concepts Explained:

#### ğŸ¤– AI Agent Workflow

An AI Agent follows a structured pipeline instead of a single LLM call:

```
ğŸ“„ Load PDF Document
        â†“
âœ‚ï¸ Split Text into Chunks
        â†“
ğŸ§¬ Generate Vector Embeddings
        â†“
ğŸ“¦ Store Embeddings in Vector Database
        â†“
ğŸ” Retrieve Relevant Context (RAG)
        â†“
ğŸ¤– LLM Generates Final Answer
```

This architecture allows the AI to **reason over external knowledge**, not just its training data.

---

#### ğŸ“„ PDF Ingestion

- A PDF file (e.g. **resume.pdf**) is loaded from the `data/` folder
- Text is extracted using a PDF parser
- This extracted text becomes the **knowledge source** for the AI

---

#### âœ‚ï¸ Chunking Strategy

- Large documents cannot be sent directly to LLMs
- Text is split into smaller overlapping chunks
- Chunk overlap ensures **context continuity**
- Each chunk is treated as an independent knowledge unit

---

#### ğŸ§¬ Embeddings & Vector Store

- Each chunk is converted into a **vector embedding**
- Embeddings capture the **semantic meaning** of text
- Vectors are stored in **ChromaDB**
- Enables fast **semantic similarity search**

---

#### ğŸ” Retrieval Augmented Generation (RAG)

- User question is converted into an embedding
- Vector DB retrieves **most relevant chunks**
- Retrieved chunks are passed as **context** to the LLM
- The LLM is instructed to answer **only from this context**
- If information is missing â†’ model replies `"I don't know"`

This eliminates hallucinations and improves trustworthiness.

---

### ğŸ› ï¸ What We Built:

A **PDF-based AI Agent application** where:

- A resume PDF is loaded from the local filesystem
- The resume content is chunked and embedded
- Embeddings are stored in a vector database
- User can ask natural language questions like:
  - _"What technologies does this person know?"_
  - _"What is the work experience?"_
  - _"Does the candidate know Python?"_
- The AI answers **only using the resume content**
- If the answer is not present in the PDF, it responds honestly

---

## ğŸ’» Tech Stack

| Technology                                                                                               | Description                                                            |
| -------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------- |
| ![Node.js](https://img.shields.io/badge/Node.js-339933?style=flat&logo=node.js&logoColor=white)          | JavaScript runtime used to build the backend GenAI application         |
| ![TypeScript](https://img.shields.io/badge/TypeScript-3178C6?style=flat&logo=typescript&logoColor=white) | Strongly typed JavaScript for scalable and maintainable code           |
| ![LangChain](https://img.shields.io/badge/LangChain-0B5ED7?style=flat)                                   | Framework for building LLM-powered applications and AI agents          |
| ![OpenAI](https://img.shields.io/badge/OpenAI-412991?style=flat&logo=openai&logoColor=white)             | Used for embeddings generation and LLM-based responses                 |
| ![ChromaDB](https://img.shields.io/badge/ChromaDB-FF6F00?style=flat)                                     | Vector database for storing and retrieving semantic embeddings         |
| ![Vector Embeddings](https://img.shields.io/badge/Vector_Embeddings-4CAF50?style=flat)                   | Numerical representation of text used for semantic search              |
| ![RAG](https://img.shields.io/badge/RAG-Retrieval_Augmented_Generation-blue?style=flat)                  | Technique to generate answers grounded in external document data       |
| ![PDF Parsing](https://img.shields.io/badge/PDF_Parsing-FF0000?style=flat)                               | Extracts raw text content from PDF documents                           |
| ![Text Chunking](https://img.shields.io/badge/Text_Chunking-7952B3?style=flat)                           | Splits large documents into manageable overlapping chunks              |
| ![AI Agent](https://img.shields.io/badge/AI_Agent-FF9800?style=flat)                                     | Agent-style workflow combining retrieval, reasoning, and generation    |
| ![dotenv](https://img.shields.io/badge/dotenv-ECD53F?style=flat&logo=dotenv&logoColor=black)             | Manages environment variables securely                                 |
| ![Chroma Retriever](https://img.shields.io/badge/Semantic_Search-2196F3?style=flat)                      | Retrieves the most relevant document chunks based on vector similarity |
| ![Prompt Template](https://img.shields.io/badge/Prompt_Templates-9C27B0?style=flat)                      | Structured prompts to control and ground LLM responses                 |

---

### ğŸ¯ Why This Project Is Important:

This final project demonstrates **real-world GenAI architecture**, used in:

- Resume analyzers
- Knowledge assistants
- Internal company chatbots
- Document Q&A systems
- AI agents with memory

It combines **LangChain + RAG + Vector Databases + LLMs** into a single, production-ready workflow.

---

### ğŸ§  Interview Ready Explanation:

> â€œI built a PDF-based AI Agent using LangChain, where documents are chunked, embedded into a vector database, retrieved using semantic search, and passed to an LLM via a RAG pipeline to generate grounded answers.â€

---

### ğŸš€ Final Outcome:

By the end of this lecture, you will have:

âœ… A complete understanding of **RAG systems**  
âœ… Hands-on experience with **AI Agents**  
âœ… A **portfolio-ready GenAI project**  
âœ… Strong foundation for advanced topics like:

- Multi-document RAG
- Conversational memory
- Tool-using AI agents
- API & UI based GenAI apps

---

ğŸ‰ **This marks the completion of the core Generative AI learning journey with Node.js & LangChain.**
